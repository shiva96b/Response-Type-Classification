{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2ibTJY68y8n"
      },
      "source": [
        "# Assignment P2: Response Type Classification in Discussions\n",
        "This notebook illustrates the Assignment P2 of CSC 791 Natural Language Processing Fall 2020. In this assignment, you will learn feature engineering through a classification task. Background Interactions through question-answering play an important role in discussions. Through questioning, askers may want to elicit information (e.g., wh-questions), clarify situations (e.g., closed-ended questions), or even make a point (e.g., rhetorical questions). However, how a question is responded does not necessarily align with the intent of the asker.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOfPFujw9HWb",
        "outputId": "5b3db1b5-7989-4eb9-c873-4df3564e9ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "#Download Spacy \n",
        "!python -m spacy download en_core_web_md\n",
        "!python3 -m spacy download en_core_web_sm\n",
        "!python -m spacy.en.download all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.2.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051305 sha256=c3acd99ec4c16404e352843d12b4630754defb9f9eaa1ade5b80e3bb722ad14d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vq2l8ms3/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "/usr/bin/python3: Error while finding module specification for 'spacy.en.download' (ModuleNotFoundError: No module named 'spacy.en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMaNOJMk9fWT",
        "outputId": "8b6ad1a3-3b32-4ed5-eaa9-667e51a297f6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Upload Testing and Training datasets\n",
        "# Please select both the files: p2_train and p2_test\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-53abb659-69c6-4899-a010-b803da572688\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-53abb659-69c6-4899-a010-b803da572688\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving p2_test.csv to p2_test.csv\n",
            "Saving p2_train.csv to p2_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVVvkXU1-op3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDo1Ddn7_AJ8"
      },
      "source": [
        "# Load Training and Testing data\n",
        "df_train = pd.read_csv(io.BytesIO(uploaded['p2_train.csv']))\n",
        "df_test  = pd.read_csv(io.BytesIO(uploaded['p2_test.csv']))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhVFgurMe6GA",
        "outputId": "a09b60de-73e3-4b27-d8ee-6c349575f57a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(df_train.shape)\n",
        "print(df_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1640, 10)\n",
            "(410, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVWYBSjXtRCa",
        "outputId": "ffab7463-79d1-4057-c018-e169cb295196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>thread_id</th>\n",
              "      <th>question_id</th>\n",
              "      <th>response_id</th>\n",
              "      <th>no_turn_q_id</th>\n",
              "      <th>quoted_q_id</th>\n",
              "      <th>precedent</th>\n",
              "      <th>question</th>\n",
              "      <th>subsequent</th>\n",
              "      <th>response</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>232fcl</td>\n",
              "      <td>232fcl</td>\n",
              "      <td>cgsrxd3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>q_27319</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Now if Julie was underage (let's say you and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\\nNo. She's happy, he's happy, who am I to end...</td>\n",
              "      <td>irrelevant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>221bir</td>\n",
              "      <td>cgikp7a</td>\n",
              "      <td>cginbgy</td>\n",
              "      <td>n_228942</td>\n",
              "      <td>q_28801</td>\n",
              "      <td>And Egypt...there was a lot of chaos, a lot of...</td>\n",
              "      <td>Did the protests motivate the military to act?</td>\n",
              "      <td>Undoubtedly. But it was still the military tha...</td>\n",
              "      <td>Seems a weird example to use in defence of an ...</td>\n",
              "      <td>attacked</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20s1x1</td>\n",
              "      <td>cg6ay9r</td>\n",
              "      <td>cg6f18i</td>\n",
              "      <td>n_244708</td>\n",
              "      <td>q_30570</td>\n",
              "      <td>&amp;gt; Hello! Both my major AND my minor are due...</td>\n",
              "      <td>Can you illustrate how a single class in CS w...</td>\n",
              "      <td>Your statement that way more happiness is ach...</td>\n",
              "      <td>Oh gods yes. First of all, most students aren'...</td>\n",
              "      <td>answered</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1zyf3k</td>\n",
              "      <td>cfzfpej</td>\n",
              "      <td>cfzg0hy</td>\n",
              "      <td>n_254761</td>\n",
              "      <td>q_31795</td>\n",
              "      <td>Oh, come on, now you're just trying to misunde...</td>\n",
              "      <td>Didn't I even say, that I am completely and 10...</td>\n",
              "      <td>Let me make it very clear: I am an universal ...</td>\n",
              "      <td>Sure, but you still obviously have some precon...</td>\n",
              "      <td>irrelevant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1yfntu</td>\n",
              "      <td>cfka1pw</td>\n",
              "      <td>cfkar3s</td>\n",
              "      <td>NaN</td>\n",
              "      <td>q_34113</td>\n",
              "      <td>NaN</td>\n",
              "      <td>However if we forget that debate for a minute...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'd be willing to guess that this question is ...</td>\n",
              "      <td>attacked</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  thread_id  ...        type\n",
              "0    232fcl  ...  irrelevant\n",
              "1    221bir  ...    attacked\n",
              "2    20s1x1  ...    answered\n",
              "3    1zyf3k  ...  irrelevant\n",
              "4    1yfntu  ...    attacked\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QabagYw_RX6",
        "outputId": "66be3bfa-9c4f-4efe-ab45-6efa1bcd0487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Analyzing the dataset\n",
        "df_train.head()\n",
        "classification = {'agreed':0, 'answered':1, 'attacked':2, 'irrelevant':3}\n",
        "\n",
        "df_train['class'] = df_train.apply(lambda i : classification[i['type']], axis=1)\n",
        "df_test['class'] = df_test.apply(lambda i : classification[i['type']], axis=1)\n",
        "\n",
        "df_train['class'] = df_train['type']\n",
        "df_train.groupby(by = [\"type\"]).count()[\"question\"]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "type\n",
              "agreed         61\n",
              "answered      994\n",
              "attacked      299\n",
              "irrelevant    286\n",
              "Name: question, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8wZl8nOP6kR"
      },
      "source": [
        "# Cleaning Text\n",
        "\n",
        "def init():\n",
        "\n",
        "  import html \n",
        "  esc = ['&amp;', '&lt;', '&quot;', '&#x27;', '&gt;', '<>']\n",
        "\n",
        "  for index, row in df_train.iterrows():\n",
        "    for e in esc:\n",
        "      if(type(row['precedent']) == str):\n",
        "        row['precedent']  = row['precedent'].replace(e, \"\")\n",
        "      if(type(row['question']) == str):\n",
        "        row['question']   = row['question'].replace(e, \"\")\n",
        "      if(type(row['subsequent']) == str):\n",
        "        row['subsequent'] = row['subsequent'].replace(e, \"\")\n",
        "      if(type(row['response']) == str):\n",
        "        row['response']   = row['response'].replace(e, \"\")\n",
        "\n",
        "  for index, row in df_test.iterrows():\n",
        "    for e in esc:\n",
        "      if(type(row['precedent']) == str):\n",
        "        row['precedent']  = row['precedent'].replace(e, \"\")\n",
        "      if(type(row['question']) == str):\n",
        "        row['question']   = row['question'].replace(e, \"\")\n",
        "      if(type(row['subsequent']) == str):\n",
        "        row['subsequent'] = row['subsequent'].replace(e, \"\")\n",
        "      if(type(row['response']) == str):\n",
        "        row['response']   = row['response'].replace(e, \"\")\n",
        "\n",
        "  df_train[\"clean_quest\"] = df_train[\"precedent\"] + df_train[\"question\"] + df_train[\"subsequent\"] + df_train['response']\n",
        "  df_test[\"clean_quest\"]  = df_test[\"precedent\"] + df_test[\"question\"] + df_test[\"subsequent\"] + df_train['response']\n",
        "\n",
        "  X_train, Y_train = df_train['clean_quest'], df_train['type']\n",
        "  X_test,  Y_test  = df_test['clean_quest'], df_test['type']\n",
        "\n",
        "  X_train = list(X_train)\n",
        "  X_test  = list(X_test)\n",
        "  Y_train = list(Y_train)\n",
        "  Y_test  = list(Y_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlomV6L2D3n_"
      },
      "source": [
        "# Cleaning Text\n",
        "\n",
        "\n",
        "import html \n",
        "esc = ['&amp;', '&lt;', '&quot;', '&#x27;', '&gt;', '<>']\n",
        "\n",
        "for index, row in df_train.iterrows():\n",
        "  for e in esc:\n",
        "    if(type(row['precedent']) == str):\n",
        "      row['precedent']  = row['precedent'].replace(e, \"\")\n",
        "    if(type(row['question']) == str):\n",
        "      row['question']   = row['question'].replace(e, \"\")\n",
        "    if(type(row['subsequent']) == str):\n",
        "      row['subsequent'] = row['subsequent'].replace(e, \"\")\n",
        "    if(type(row['response']) == str):\n",
        "      row['response']   = row['response'].replace(e, \"\")\n",
        "\n",
        "for index, row in df_test.iterrows():\n",
        "  for e in esc:\n",
        "    if(type(row['precedent']) == str):\n",
        "      row['precedent']  = row['precedent'].replace(e, \"\")\n",
        "    if(type(row['question']) == str):\n",
        "      row['question']   = row['question'].replace(e, \"\")\n",
        "    if(type(row['subsequent']) == str):\n",
        "      row['subsequent'] = row['subsequent'].replace(e, \"\")\n",
        "    if(type(row['response']) == str):\n",
        "      row['response']   = row['response'].replace(e, \"\")\n",
        "\n",
        "df_train[\"clean_quest\"] = df_train[\"precedent\"] + df_train[\"question\"] + df_train[\"subsequent\"] + df_train['response']\n",
        "df_test[\"clean_quest\"]  = df_test[\"precedent\"] + df_test[\"question\"] + df_test[\"subsequent\"] + df_train['response']\n",
        "\n",
        "X_train, Y_train = df_train['clean_quest'], df_train['type']\n",
        "X_test,  Y_test  = df_test['clean_quest'], df_test['type']\n",
        "\n",
        "X_train = list(X_train)\n",
        "X_test  = list(X_test)\n",
        "Y_train = list(Y_train)\n",
        "Y_test  = list(Y_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYmv-X5vWg4x"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX0KOBbjWgJJ",
        "outputId": "1694189e-5e4e-4f00-cfe3-af7a839eb1f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "init()\n",
        "# Sentence Embedding\n",
        "\n",
        "# Load Spacy model\n",
        "import spacy.cli\n",
        "\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjFs_PHlWgOP"
      },
      "source": [
        "# For Spacy word2vec\n",
        "def createVector(sent):\n",
        "  vect = nlp(sent)\n",
        "  avg = np.zeros(300)\n",
        "  # print(vect)\n",
        "  \n",
        "  # Calculate average vector\n",
        "  for token in vect: \n",
        "    avg += token.vector\n",
        "  vect_avg = avg/len(vect)\n",
        "\n",
        "  return vect_avg.tolist()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akcatFPQWgVc"
      },
      "source": [
        "# Apply Transformation\n",
        "df_train['vector'] = df_train.apply(lambda i : createVector(str(i['clean_quest'])), axis=1)\n",
        "df_test['vector']  = df_test.apply(lambda i  : createVector(str(i['clean_quest'])), axis=1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frt45q5EWgaS"
      },
      "source": [
        "# Trasforming Vector info\n",
        "\n",
        "X_train = np.array(df_train['vector'].values.tolist())\n",
        "X_test  = np.array(df_test['vector'].values.tolist())\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-EuLOkuWgYH",
        "outputId": "ec902fb5-86b7-48ae-9d6a-0a56b01702e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "prediction1 = classifier.predict(X_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a07u0Xfm-8-o"
      },
      "source": [
        "def accuracy(Y_test, prediction):\n",
        "  same = 0 \n",
        "  for i,j in zip(Y_test, prediction):\n",
        "    if i==j:\n",
        "      same += 1\n",
        "  return (same/len(Y_test))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkfurWQL9C7O",
        "outputId": "3951c894-14fa-4363-ce98-ddba8a95aba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# Temporary splitting of testing and training data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(Y_test, prediction1))\n",
        "print(classification_report(Y_test, prediction1))\n",
        "print(\"Accuracy: \", accuracy(Y_test, prediction1))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0  13   0   0]\n",
            " [  0 310   7   3]\n",
            " [  0  37   2   0]\n",
            " [  0  37   1   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      agreed       0.00      0.00      0.00        13\n",
            "    answered       0.78      0.97      0.86       320\n",
            "    attacked       0.20      0.05      0.08        39\n",
            "  irrelevant       0.00      0.00      0.00        38\n",
            "\n",
            "    accuracy                           0.76       410\n",
            "   macro avg       0.25      0.26      0.24       410\n",
            "weighted avg       0.63      0.76      0.68       410\n",
            "\n",
            "Accuracy:  0.7609756097560976\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiKa5SN5-05L"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hY74pAjYzEs",
        "outputId": "f268df48-416f-4f18-8151-5b0dc4ab6cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Install vaderSentiment\n",
        "!pip install vaderSentiment"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2020.6.20)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYlih20MWgTB"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "init()\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "import string\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def sentiment_analyzer_scores(text):\n",
        "    score = analyzer.polarity_scores(text)\n",
        "    return score['compound']\n",
        "\n",
        "# {'neg': 0.197, 'neu': 0.754, 'pos': 0.049, 'compound': -0.9764}: Example Output"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PO30VQ4WgRH"
      },
      "source": [
        "X_train, X_test = [], []\n",
        "\n",
        "import six\n",
        "\n",
        "for i in df_train['clean_quest']:\n",
        "  if isinstance(i, six.string_types):\n",
        "    X_train.append([sentiment_analyzer_scores(i)])\n",
        "  else:\n",
        "    X_train.append([0])\n",
        "\n",
        "for i in df_test['clean_quest']:\n",
        "  if isinstance(i, six.string_types):\n",
        "    X_test.append([sentiment_analyzer_scores(i)])\n",
        "  else:\n",
        "    X_test.append([0])\n",
        "\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRIPZ7RGWgMB"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "classifier.fit(X_train, Y_train)\n",
        "prediction2 = classifier.predict(X_test)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FyZNECb_2b2",
        "outputId": "631d9dc2-0d02-4dd3-86df-aa1f78fa99da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Temporary splitting of testing and training data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(Y_test, prediction2))\n",
        "print(classification_report(Y_test, prediction2))\n",
        "print(\"Accuracy: \", accuracy(Y_test, prediction2))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0  12   1   0]\n",
            " [  0 304  14   2]\n",
            " [  1  35   2   1]\n",
            " [  1  35   2   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      agreed       0.00      0.00      0.00        13\n",
            "    answered       0.79      0.95      0.86       320\n",
            "    attacked       0.11      0.05      0.07        39\n",
            "  irrelevant       0.00      0.00      0.00        38\n",
            "\n",
            "    accuracy                           0.75       410\n",
            "   macro avg       0.22      0.25      0.23       410\n",
            "weighted avg       0.62      0.75      0.68       410\n",
            "\n",
            "Accuracy:  0.7463414634146341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RWAdOMa_2jx"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhTHE60xxvFn",
        "outputId": "b42772f4-1809-460d-f3a0-656a8b49535c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "from nltk import word_tokenize\n",
        "from nltk.data import load\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNK7VPMbxvPM"
      },
      "source": [
        "# POS Tagging\n",
        "# Creating a one-hot encoder vector to represent the information\n",
        "\n",
        "init()\n",
        "\n",
        "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
        "pos_tags_list = list(tagdict.keys())\n",
        "pos_dict = dict.fromkeys(pos_tags_list, 0)\n",
        "\n",
        "X_train, X_test = [], []\n",
        "\n",
        "def tagger(df):\n",
        "  res = []\n",
        "  for sen in df['clean_quest']:\n",
        "    tags = nltk.pos_tag(word_tokenize(str(sen)))\n",
        "    sen_dict = pos_dict.copy()\n",
        "    for w,t in tags:\n",
        "      if t in sen_dict:\n",
        "        sen_dict[t] += 1\n",
        "    res.append(list(sen_dict.values()))\n",
        "  return res\n",
        "\n",
        "X_train, X_test = np.array(tagger(df_train)), np.array(tagger(df_test))\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-Dy_2vsxvDL",
        "outputId": "b21b24a5-2847-4a76-9c4b-b4dd67f7381d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "classifier = LinearSVC()\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "prediction3 = classifier.predict(X_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjU-Qhxw2r1X",
        "outputId": "e5a0f79f-3c7e-427d-ab7a-8df90895d5d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# Temporary splitting of testing and training data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(Y_test, prediction3))\n",
        "print(classification_report(Y_test, prediction3))\n",
        "print(\"Accuracy: \", accuracy(Y_test, prediction3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0  13   0   0]\n",
            " [  0 315   1   4]\n",
            " [  0  39   0   0]\n",
            " [  0  37   0   1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      agreed       0.00      0.00      0.00        13\n",
            "    answered       0.78      0.98      0.87       320\n",
            "    attacked       0.00      0.00      0.00        39\n",
            "  irrelevant       0.20      0.03      0.05        38\n",
            "\n",
            "    accuracy                           0.77       410\n",
            "   macro avg       0.24      0.25      0.23       410\n",
            "weighted avg       0.63      0.77      0.68       410\n",
            "\n",
            "Accuracy:  0.7707317073170732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQNPZcjN2ryf"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2_30QTH2rtc"
      },
      "source": [
        "# Self defined Decission Tree Classifier\n",
        "\n",
        "# Using the sentiment score to check for discussions which lead classify as 'attacked'\n",
        "# Using lexicon based approach after reviewing elements in the training data set for the other 3 classes\n",
        "\n",
        "init()\n",
        "\n",
        "sentiment_score, prediction4 = [], []\n",
        "agreed = ['yup', 'yes','correct','rational','sure','absolutely','agree']\n",
        "answered = ['read','actually','fact','think']\n",
        "\n",
        "for i in df_test['clean_quest']:\n",
        "  if isinstance(i, six.string_types):\n",
        "    sentiment_score.append([sentiment_analyzer_scores(i)])\n",
        "  else:\n",
        "    sentiment_score.append([0])\n",
        "\n",
        "def check_if_exists( sentence, lst):\n",
        "  for i in lst:\n",
        "    if isinstance(sentence, six.string_types):\n",
        "      if i in sentence:\n",
        "        return True\n",
        "  return False\n",
        "\n",
        "i=0\n",
        "for index, row in df_test.iterrows():\n",
        "  if sentiment_score[i][0] < (-0.5):\n",
        "    prediction4.append('attacked')\n",
        "  elif check_if_exists( row['clean_quest'], agreed):\n",
        "    prediction4.append('agreed') \n",
        "  elif check_if_exists( row['clean_quest'], answered):\n",
        "    prediction4.append('answered')\n",
        "  else:\n",
        "    prediction4.append('irrelevant')\n",
        "  i += 1\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuJyG5POJsHp",
        "outputId": "8434f212-2649-4c77-c741-a77ee4883c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(confusion_matrix(Y_test, prediction4))\n",
        "print(classification_report(Y_test, prediction4))\n",
        "print(\"Accuracy: \", accuracy(Y_test, prediction4))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  1   3   8   1]\n",
            " [ 37  55  73 155]\n",
            " [  6   3  13  17]\n",
            " [  2   6   7  23]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      agreed       0.02      0.08      0.03        13\n",
            "    answered       0.82      0.17      0.28       320\n",
            "    attacked       0.13      0.33      0.19        39\n",
            "  irrelevant       0.12      0.61      0.20        38\n",
            "\n",
            "    accuracy                           0.22       410\n",
            "   macro avg       0.27      0.30      0.18       410\n",
            "weighted avg       0.66      0.22      0.26       410\n",
            "\n",
            "Accuracy:  0.22439024390243903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGiUk0pCaqJl"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiWyXscqEE17"
      },
      "source": [
        "# Ensemble Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1KC0KllAWyS"
      },
      "source": [
        "import operator\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "classification = {'agreed':0, 'answered':1, 'attacked':2, 'irrelevant':3}\n",
        "predictionA, predictionB = [], []\n",
        "\n",
        "for i in range(len(prediction1)):\n",
        "  result1 = {'agreed':0, 'answered':0, 'attacked':0, 'irrelevant':0}\n",
        "  result2 = {'agreed':0, 'answered':0, 'attacked':0, 'irrelevant':0}\n",
        "\n",
        "  result1[prediction1[i]] += 0.5\n",
        "  result1[prediction2[i]] += 0.6\n",
        "  result1[prediction3[i]] += 0.3\n",
        "  result1[prediction4[i]] += 0.15\n",
        "\n",
        "  max_value = max(result1.values())\n",
        "  for cls in result1.keys():\n",
        "    if( result1[cls] == max_value):\n",
        "      predictionA.append(cls)\n",
        "\n",
        "  result2[prediction1[i]] += 0.5\n",
        "  result2[prediction3[i]] += 0.4\n",
        "\n",
        "  max_value = max(result2.values())\n",
        "  for cls in result2.keys():\n",
        "    if( result2[cls] == max_value):\n",
        "      predictionB.append(cls)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgLeiYmJJlFO",
        "outputId": "c2a0afc9-03eb-48e3-a3f8-d788a67bd322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(Y_test, predictionB))\n",
        "print(classification_report(Y_test, predictionB))\n",
        "print(\"Accuracy: \", accuracy(Y_test, predictionB))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0  13   0   0]\n",
            " [  0 310   7   3]\n",
            " [  0  37   2   0]\n",
            " [  0  37   1   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      agreed       0.00      0.00      0.00        13\n",
            "    answered       0.78      0.97      0.86       320\n",
            "    attacked       0.20      0.05      0.08        39\n",
            "  irrelevant       0.00      0.00      0.00        38\n",
            "\n",
            "    accuracy                           0.76       410\n",
            "   macro avg       0.25      0.26      0.24       410\n",
            "weighted avg       0.63      0.76      0.68       410\n",
            "\n",
            "Accuracy:  0.7609756097560976\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kwMWhNMAWuM",
        "outputId": "a4b4424e-50de-4437-ee29-16d2a7432df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(Y_test, predictionA))\n",
        "print(classification_report(Y_test, predictionA))\n",
        "print(\"Accuracy: \", accuracy(Y_test, predictionA))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0  13   0   0]\n",
            " [  0 319   1   0]\n",
            " [  0  39   0   0]\n",
            " [  0  37   1   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      agreed       0.00      0.00      0.00        13\n",
            "    answered       0.78      1.00      0.88       320\n",
            "    attacked       0.00      0.00      0.00        39\n",
            "  irrelevant       0.00      0.00      0.00        38\n",
            "\n",
            "    accuracy                           0.78       410\n",
            "   macro avg       0.20      0.25      0.22       410\n",
            "weighted avg       0.61      0.78      0.68       410\n",
            "\n",
            "Accuracy:  0.7780487804878049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDaisqVHBsqY"
      },
      "source": [
        "# References:\n",
        "\n",
        "\n",
        "*   https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n",
        "*   https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
        "*   https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
        "*   https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T30N8uFCeCHF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}